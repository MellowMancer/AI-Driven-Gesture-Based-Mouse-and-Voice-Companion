<h2>Description</h2>

The rise of fields such as AI and ML have provided a platform to create solutions to make the world of technology as inclusive and accessible as possible. But for the specially abled or aged people, using a laptop or desktop is not their cup of tea. They might not know how to use their laptop or desktop and are not at par with other people in making full use of technology.

This system incorporates an AI-powered hand gesture interface and a voice assistant to simulate a computer mouse, and various keyboard shortcuts and perform multiple voice-operated functions. By utilizing computer vision, hand detection algorithms, natural language processing, and voice-to-text algorithms, this system enables users to interact with computers and projectors remotely through movements of their hand and various inbuilt gestures, eliminating the need for physical peripherals. 

This innovative approach has the potential to redefine human-computer interaction and make it more accessible than ever before. 
Simply run the file main.py to get the project working.

<h2>Tech stack: </h2>
<ul>
  <p><strong>Backend: </strong> Python, OpenCV, Mediapipe, SpeechRecognition, pyttsx3</p>
  <p><strong>GUI: </strong> PyQt6</p>
</ul>

<h2>Applications: </h2>
<ul>
  <li>It can be used during laboratories and while cooking, enabling a touchless interface for the users.</li>
  <li>It can also be used by the physically disabled or aged people to create a new path for them to interact with the existing technologies.</li>
  <li>The project was chiefly inspired during COVID times where limiting contact was the main goal.</li>
</ul>

<h2>Research Paper Publication</h2>

Check out the research paper published in 6th IEEE-International Conference on Advances in Science and Technology (ICAST) <a href="https://ieeexplore.ieee.org/document/10454959">here</a>.

<h2>Demonstration Video: </h2>

[![AI-Driven-Gesture-Based-Mouse-and-Voice-Companion](https://img.youtube.com/vi/ZabwoOmgybw/0.jpg)](https://www.youtube.com/watch?v=ZabwoOmgybw)

